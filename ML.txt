1. 
 
import csv attributes=[['Sunny','Cloudy','Rainy'], 
            ['Warm','Cold'], 
            ['Normal','High'], 
            ['Strong','Weak'], 
            ['Warm','Cool'], 
            ['Same','Change']] 
 
total_attributes=len(attributes) print("\nTotal number of attributes is:",total_attributes) print("The most specific hypothesis:['0','0','0','0','0','0']") print("The most general hypothesis:['?','?','?','?','?','?']") a=[] print("\n the given training Data set is:") with open('EnjoySport.csv','r') as cfile:     for row in csv.reader(cfile):         a.append(row)         print(row) print("\nTotal number of records is:",len(a)) print("The initial Hypothesis is:") hypothesis=['0']*total_attributes print(hypothesis) 
 
#comparing with tarining examples of given dataset for i in range(0,len(a)):     if a[i][total_attributes]=='Yes':         for j in range(0,total_attributes):             if hypothesis[j]=='0' or hypothesis[j]==a[i][j]: 
                hypothesis[j]=a[i][j]             else: 
                hypothesis[j]='?'         print("\n Hypothesis for training example n0 {} is:\n" .format(i+1),hypothesis) print("\n The maximally specific Hypothesis for s given training examples:") print(hypothesis) 
 	 
 	 
 	 
 	 
 	 
 	 
 	 
 	 
 	 
  
 
 
2. 
import csv with open('EnjoySport.csv') as cfile: 
    examples = [ tuple(row) for row in csv.reader(cfile) ] 
  
  
def getdomains(examples): 
    d = [ set() for i in examples[0] ]     for r in examples: 
        for c, v in enumerate(r):d[c].add(v)         return [ list(sorted(x)) for x in d ] 
 
  
def g0(n):     return ('?',)*n 
 
 
def s0(n): 
    return ('0',)*n 
 
 
def more_general(h, e): 
    mgparts = []     for x, y in zip(h, e):         mg = x == '?' or (x != '0' and (x == y or y == "0"))         mgparts.append(mg)     return all(mgparts) 
 
 
def consistent(hypothesis, example): 
    return more_general(hypothesis, example) 
 
  
def min_generalizations(s, e): 
    s_new = list(s)     for i in range(len(s)):         if not consistent(s[i:i+1],e[i:i+1]):             if s[i] != '0': 
                s_new[i] = '?'             else: 
                s_new[i] = e[i]     return [tuple(s_new)] 
 
 
def generalize_S(e, G, S):     S_prev = list(S)     for s in S_prev:         if s not in S:             continue         if not consistent(s,e): 
            S.remove(s) 
            Splus = min_generalizations(s, e)     S.update( [ h for h in Splus 
               if any( [ more_general(g, h) for g in G ] ) ] ) 
    S.difference_update( [ h for h in S if any( [ more_general(h, h1) for h1 in S if h != h1 ])])     return S 
 
 
def min_specializations(h, domains, e):     results = []     for i in range(len(h)):         if h[i] == '?':             for val in domains[i]:                 if e[i] != val: 
                    h_new = h[:i] + (val, ) + h[i+1:]                     results.append(h_new)         elif h[i] != '0': 
            h_new = h[: i] + ('0', ) + h[i+1:]             results.append(h_new)     return results 
 
 
def specialize_G(e, domains, G, S):     G_prev = list(G)     for g in G_prev:         if g not in G: 
            continue         if consistent(g, e): 
            G.remove(g) 
    Gminus = min_specializations(g, domains, e) 
    G.update( [ h for h in Gminus if any( [ more_general(h, s) for s in S])]) 
    G.difference_update( [ h for h in G if any( [ more_general(g1, h) for g1 in G if h != g1 ] ) ] )     return G 
 
 
def candidate_elimination(examples):     domains = getdomains(examples)[:-1] 
    G = set( [ g0(len(domains) ) ] )     S = set( [ s0(len(domains) ) ] )     i = 0     print("\n Initially")     print("G[{0}]:".format(i), G)     print("S[{0}]:".format(i), S)     for r in examples:         i = i+1         e, t = r[:-1], r[-1]          if t == 'Yes': 
            G = {g for g in G if consistent(g, e)}             S = generalize_S(e, G, S)         else: 
            S = {s for s in S if not consistent(s, e)}             G = specialize_G(e, domains, G, S)         print("For Training example {0}".format(i))         print("G[{0}]:".format(i), G)         print("S[{0}]:".format(i), S)     return 
candidate_elimination(examples) 
 








3. 
import pandas as pd import numpy as np dataset= pd.read_csv('PlayTennis.csv',names=['outlook','temperature','humidity','wind','playtennis',]) def entropy(target_col): 
    elements,counts = np.unique(target_col,return_counts = True)     entropy = np.sum([(-counts[i]/np.sum(counts))*np.log2(counts[i]/np.sum(counts)) for i in range(len(elements))])     return entropy 
def InfoGain(data,split_attribute_name,target_name="playtennis"): 
    total_entropy = entropy(data[target_name]) 
    vals,counts= np.unique(data[split_attribute_name],return_counts=True)     Weighted_Entropy = np.sum([(counts[i]/np.sum(counts))*entropy(data.where(data[split_attribute_name]==vals[i]).dro pna()[target_name]) for i in range(len(vals))]) 
    Information_Gain = total_entropy - Weighted_Entropy     return Information_Gain 
 
def ID3(data,originaldata,features,target_attribute_name="playtennis",parent_node_class = None): 
    if len(np.unique(data[target_attribute_name])) <= 1:         return np.unique(data[target_attribute_name])[0]     elif len(data)==0:         return 
np.unique(originaldata[target_attribute_name])[np.argmax(np.unique(originaldata[target_attribut e_name],return_counts=True)[1])]     elif len(features) ==0: 
        return parent_node_class     else: 
        parent_node_class = np.unique(data[target_attribute_name])[np.argmax(np.unique(data[target_attribute_name],retur n_counts=True)[1])]     item_values = [InfoGain(data,feature,target_attribute_name) for feature in features] #Return the information gain values for the features in the dataset     best_feature_index = np.argmax(item_values)     best_feature = features[best_feature_index]     tree = {best_feature:{}}     features = [i for i in features if i != best_feature]     for value in np.unique(data[best_feature]): 
        value = value         sub_data = data.where(data[best_feature] == value).dropna()         subtree = ID3(sub_data,dataset,features,target_attribute_name,parent_node_class)         tree[best_feature][value] = subtree     return(tree) 
 
tree = ID3(dataset,dataset,dataset.columns[:-1]) print(' \nDisplay Tree\n',tree) 
 







9 
from sklearn.neighbors import KNeighborsClassifier from sklearn.metrics import confusion_matrix from sklearn.metrics import accuracy_score 
from sklearn.metrics import classification_report from sklearn.model_selection import train_test_split import pandas as pd dataset=pd.read_csv("iris.csv") 
X	= dataset[ ['Sepal-Length', 'Sepal-Width', 'Petal-Length', 'Petal-Width'] ] 
Y	= dataset['Class'] 
X_train,X_test,y_train,y_test=train_test_split(X,Y,random_state=0,test_size=0.25) 
 
classifier=KNeighborsClassifier(n_neighbors=8,p=3,metric='euclidean') classifier.fit(X_train,y_train) 
 
#predict the test resuts y_pred=classifier.predict(X_test) 
 
cm=confusion_matrix(y_test,y_pred) print('Confusion matrix is as follows\n',cm) print('Accuracy Metrics') print(classification_report(y_test,y_pred)) print(" correct predicition",accuracy_score(y_test,y_pred)) print(" worng predicition",(1-accuracy_score(y_test,y_pred))) 
 










10 
 
import matplotlib.pyplot as plt 
 
import pandas as pd 
 
import numpy as np 
 
 
 
def kernel(point, xmat, k): 
 
    m, n = np.shape(xmat) 
    weights = np.mat(np.eye((m))) # eye - identity matrix      for j in range(m):         diff = point - X[j] 
 
        weights[j, j] = np.exp(diff*diff.T/(-2.0*k**2)) 
 
    return weights 
 
 
 
def localWeight(point, xmat, ymat, k): 
 
    wei = kernel(point,xmat,k) 
 
    W = (X.T*(wei*X)).I*(X.T*(wei*ymat.T)) 
 
    return W 
 
 
 
def localWeightRegression(xmat, ymat, k): 
 
    m, n = np.shape(xmat) 
 
    ypred = np.zeros(m) 
 
    for i in range(m): 
 
        ypred[i] = xmat[i]*localWeight(xmat[i], xmat, ymat, k) 
 
    return ypred 
 
 
 
def graphPlot(X, ypred): 
 
    sortindex = X[:,1].argsort(0) #argsort - index of the smallest 
 
    xsort = X[sortindex][:,0] 
 
    fig = plt.figure()     ax = fig.add_subplot(1,1,1) 
 
    ax.scatter(bill,tip, color='green')     ax.plot(xsort[:,1],ypred[sortindex], color = 'red', linewidth=5) 
     plt.xlabel('Total bill') 
 
    plt.ylabel('Tip') 
 
    plt.show() 
 
 
 
# load data points data=pd.read_csv('Tips.csv') 
 
bill = np.array(data.total_bill) # We use only Bill amount and Tips data tip = np.array(data.tip) 
 
mbill = np.mat(bill) # .mat will convert nd array is converted in 2D array 
 
mtip = np.mat(tip) 
 
m= np.shape(mbill)[1] 
 
one = np.mat(np.ones(m)) 
 
X = np.hstack((one.T, mbill.T)) # 244 rows, 2 cols 
 
#Prediction with k=3  print('\nypred for k=3') 
 
ypred = localWeightRegression(X,mtip,3)  graphPlot(X, ypred) 
 
#Prediction with k=9 
 
print('\nypred for k=9') 
 
ypred = localWeightRegression(X, mtip, 9) graphPlot(X, ypred) 
 
 
 
graphPlot(X, ypred) 
